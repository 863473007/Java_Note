1. Elasticsearch
    - 存储结构
	    - 由 _index、_type和_id唯一标识一个文档
	    - _index指向一个或多个物理分片的逻辑命名空间，_type类型由于区分同一个集合中的不同细分，在不同的细分中，数据的整体模式是相同或相似的，不适合完全不同类型的数据，多个_type可以在相同的索引中存在，只要他们的字段不冲突即可，_id文档标记由系统自动生成或使用者提供
    - 分片概念
	    - 分布式系统中，单机无法存储规模巨大的数据，要依靠大规模集群处理和存储这些数据，一般通过增加机器数量来提高系统水平扩展能力，因此，需要将数据分成若干小块分配到各个机器上，然后通过某种路由策略找到某个数据块所在的位置
		- ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/web/Elasticsearch/image/es-1.png) 
	    - ES将数据副本分为主从两部分，即主分片和副分片，主数据作为权威数据，写过程中先写主分片，成功后在写副分片，恢复阶段以主分片为准
	    - 分片是底层的基本读写单元，分片的目的是分割巨大的索引，让读写可以并行操作
	    - 数据分片时，不仅要让节点间均匀存储，同时还要避免主分片与副分片分配到同一个节点（分配到同一个节点会引起数据丢失）
	    - 在实际应用中，不应该向单个索引持续写入数据，直到它的分片巨大无比；以_id为单位删除文档不会立刻释放空间，删除的doc只在Lucene分段合并时才会真正从磁盘中删除
		    - 建议进行周期性创建新的索引，索引别名可以指向一个或多个索引
	- 如何实现近实时搜索的
	    - 写操作中，一般会先在内存中缓冲一段数据，再将这些数据写入硬盘，每次写入硬盘的这批数据成为一个分段，如同写操作一样；一般情况下（direct）方式除外，通过操作系统write接口写到磁盘的数据先到达系统缓存（内存），write函数返回成功时，数据未必被刷到磁盘，通过手工调用flush，或者操作系统通过一定策略将系统缓存刷到磁盘，这种策略大幅提升了写入效率，从write函数返回成功开始，无论数据有没有被刷到磁盘，该数据已对读取可见。
	    - 如何解决数据丢失的问题
		    - 采用记录事务日志，每次对ES进行操作时均记录事务日志，当ES启动的时候，重放translog中所有在最后一次提交后发生的变更操作
    -  集群
	    - 至少存在主节点和数据节点，另外还有协调节点、预处理节点和部落节点
	    - 主节点也可以作为数据节点，但是尽可能做少量的工作，生产环境中尽量分离主节点和数据节点
	    - 数据节点一般情况下，数据读写流程之和数据节点打交道，不会和主节点打交道（异常除外）
	    - 协调节点，客户端请求可以发送到集群的任何节点，每个节点都知道任意文档所处的位置，然后转发这些请求，处理客户端请求的节点称为协调节点；协调节点将请求转发给保存数据的数据节点，需要对每个数据节点的结果合并为单个全局结果
	    - 集群启动的流程
		    - 集群的启动过程指集群完全重启时的启动阶段，期间要经历选举主节点、主分片、数据恢复等重要阶段
		    - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/web/Elasticsearch/image/es-2.png)
		    - 如何选主分片
			    - Master向集群中的所有节点询问：大家把[website][0]分片的原信息发送给我，然后master等待所有的请求返回
		    - Index recovery流程（类似Raft协议？）
			    - 分片分配成功后进入recovery流程，主分片的recovery不会等待其副分片分配成功后才开始recovery，是独立的流程，只是副分片的recovery需要主分片恢复完毕才开始
			    - 主分片recovery
				    - 每次写操作都会记录事务日志，事务日志中记录了哪种操作以及相关的数据，最后一次提交（Lucene的一次提交就是一次fsync刷盘的过程）之后的translog的重放，建立Lucene索引，如此完成主分片的recovery操作
				    - 主分片负责维护global checkpoint，代表所有分片都已写入这个序号的位置，local checkpoint代表分片已写入成功的最新位置，恢复时通过对比这两个序号，计算出缺失的数据范围，然后通过translog重放这部分数据，同时translog会为此保留更长时间
			    - 副分片recovery
				    - 副分片需要恢复成与主分片一致，同时，恢复期间允许新的索引操作。恢复阶段分成两部分进行
				    - 第一阶段：在主分片所在的节点，获取translog保留锁，从获取保留锁开始，会保留translog不受其刷盘清空的影响，然后调用Lucene接口把shard做快照，这是已经刷磁盘中的数据，然后把shard数据复制到副本节点，在此步骤完毕之前，会向副分片节点告知对方启动engine，在第二阶段开始之前，副分片就可以正常处理写请求了（在正常的写操作中，每次写入成功的操作都分配一个序号）
				    - 第二阶段：对translog做快照，快照里包含从第一阶段开始，到执行translog快照期间的新增索引。将这些translog发送到副分片所在的节点进行重放
					    - 分片数据的完整性：如果做到副分片数据不丢失（阻止刷新操作，保留translog；采用translog.view；采用TranslogDeletionPolicy，对translog做一个快照来保持translog不被清理）
					    - 数据一致性：当出现时序错误与冲突的时候，通过对比版本号来过滤掉过期操作，保证了只有最新的一次操作生效，保证了主副分片的一致性
	    - 集群选主流程
		    - 选举算——Bully算法：它假定所有的节点都有一个唯一的ID，使用该ID对节点进行排序，任何时候的当前Leader都是参与集群的最高ID节点；通过推迟选举，直到当前的Master失效来解决上述问题，只要当前主节点不挂掉，就不重新选主，但是容易产生脑裂（双主），为此，再通过”法定得票人数过半“来解决脑裂的问题
		    - 流程概述（Zen Discovery）
			    - 每个节点计算最小的已知节点ID，该节点为临时Master，向该节点发送领导投票
			    - 如果一个节点收到足够多的票数，并且该节点也为自己投票，那么它将扮演领导者的角色，开始发布集群状态
			    - 所有节点都会参与集群选举，并参与投票，但是，只有有资格成为Master的节点的投票才有效
			    - 临时Master选举如下
				    - Ping 所有节点，获取节点列表的返回结果，ping结果不包含本节点，把本节点单独添加到响应结果中
				    - 构建两个列表
					    - activeMasters列表：存储集群当前获取的master列表
					    - masterCandidates列表：存储Master候选者列表
					    - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/web/Elasticsearch/image/es-3.png)
						- ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/web/Elasticsearch/image/es-4.png)
	    - 数据副本策略
		    - 分片副本使用主从模式，多个副本中存在一个主副本和多个从副本，所有的数据写入操作都进入主副本，当主副本出现故障无法访问时，系统从其他副本中选择合适的副本作为新的主副本
		    - 数据写入流程
			    - 写请求进入主副本节点，节点为该操作分配SN，使用该SN创建UpdateRequest结构，将该UpdateRequest插入自己的prepare list中
			    - 主副本节点将携带SN的UpdateRequest发往从副本节点，从节点收到后插入到prepare list，完成后给主副本节点回复一个ACK
			    - 一旦主副本节点收到所有从副本节点的响应，确定该数据已经被正确写入所有的从副本节点，此时认为可以提交了
			    - 主副本节点回复客户端更新成功完成，对每一个prepare消息，主副本节点向从副本节点发送一个commit通知，告诉自己的commited point的位置，从副本节点收到通知后根据指示移动commited point到相同的位置
			    - 主副本只有在从副本节点将请求添加进prepare list之后才可以通过移动commited point的方式将该请求插入commited list中
		    - 基本写入模型
			    - ES维护一个可以接收该操作的分片的副本列表，这个列表叫做同步副本列表，有master节点进行维护
			    - 请求到达协调节点，协调节点先进行验证操作，有错就拒绝该操作，然后根据集群状态，请求被路由到主分片所在的节点
			    - 操作在主分片上本地执行
			    - 操作成功后，转发该操作到当前in-sync副本组的所有副本，如果有多个副分片，则并行进行转发
			    - 所有副分片节点完成操作并回复主分片节点后，主分片节点会把请求执行成功的信息返回给协调节点，协调节点返回给客户端
		    - 分片的唯一标识：allocation ids
			    - 由主节点在进行分片分配时指定，并由数据节点存储在磁盘种
			    - 分配主分片时，主节点检查磁盘中存储的allocation id是否会在集群状态的in-sync allocations ids集合中出现，只有在这个集合中找到了，此分片才有可能被选为主分片（类似checkpoint机制）
			    - 主分片负责推进全局检查点，跟踪在副分片上的完成的操作来实现，一旦检测到所有的副分片已经超出给定的序列号，将相应的更新全局检查点。副分片维护一个本地检查点，所有序列号低于它的操作都已经在该分片上处理（Lucene以及translog写成功，但是不保证刷盘）完毕
		    - 全局检查点
			    - 为了避免大量的操作需要进行比对检查，采用了一个全局检查点的安全标记，全局检查点是所有活跃分片历史都已对齐的序列号，所有低于全局检查点的操作都保证已被所有活跃的分片处理完毕，当主分片失效时，只需要比较新主分片与其他副分片之间的最后一个全局检查点之后的操作即可
	    - 错误检测
		    - 为了解决新旧主副本同时存在的问题，采用了租约机制来解决，主副本定期向其他从副本获取租约
			    - 如果主副本在一定的时间内未收到从副本节点的租约回复，则主副本节点认为从副本节点异常，向配置管理器汇报，将该异常从副本从副本组中移除，同时它自己也降级，不再作为主副本节点
			    - 从副本在一定时间内未收到主副本节点的租约请求，则认为主副本异常，向配置管理汇报，将主副本从副本组中移除，同时将自己提升为新的主副本节点，哪个从副本先执行成功，哪个从副本就提升为主副本节点
    - 文档
	    - 每个文档都有一个版本号（version），当文档被修改时版本号递增，ES使用这个_version来确保变更以正确的顺序执行，如果旧版本的文档在新版本特性之后到达，可以被忽略
	    - 版本号由主分片生成
	    - 版本号的另一个作用是实现乐观锁，在写请求中指定文档版本号，如果文档中的当前版本与请求中指定的版本号不同，则会请求失败
