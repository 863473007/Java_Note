1. Netty
    - Netty的线程模型（Netty可以同时支持Reactor但单线程模型、多线程模型和主从Reactor多线程模型）
        - ![](https://github.com/chuntaojun/Java_Note/blob/master/web/netty/image/netty-1.jpg)
        - ![](https://github.com/chuntaojun/Java_Note/blob/master/web/netty/image/netty-2.jpg)
        - ![](https://github.com/chuntaojun/Java_Note/blob/master/web/netty/image/netty-3.jpg)
	    -（reactor多线程模式）专门的NIO线程——Acceptor线程监听服务端，接受客户端的TCP请求；（主从reactor多线程模式）处理客户端的连接不在由一个线程处理，而是转为有一个线程池处理（NioEventLoop持有一个Selector）
		v. 网络I/O操作——读、写由一个NIO线程池负责，复制消息的编解码、读取、发送
    - NioEventLoop
        - 封装各种Task，实现局部无锁化（I/O线程与用户线程同时操作网络资源时，将用户线程操作封装成Task放入消息队列中，task之间顺序执行，由I/O线程复负责执行）
        - netty解决selector导致的epoll bug的解决方案
		    - 起因：selector轮询为空，没有wakeup操作或是新的消息需要处理，则本次是一个空轮询
		    - 方案：
			    - 对selector的select操作周期进行统计，每完成一次空轮询进行一次计数，在某个周期内连续发生N次空轮询，触发bug
			    - 重建selector，rebuildSelector任务封装成Task由NioEventLoop进行调用
			    - 将旧的selector上注册的SocketChannel转注册到新的selector，并从旧selector去注册，关闭旧selector
        - newChooser()——实现将连接绑定到对应的EventLoop中（实现主从Reactor线程模型）
            - 如何平衡I/O事件与非I/O事件
                - 通过设置ioRatio（判断I/O操作与定时任务和Task的数量多少关系）
                - 根据时间戳对定时任务进行判断，如果正在处于或者已经处于超时状态，将其加入到执行TaskQueue中，并且延时队列中删除（在添加任务时，如果是在EventLoop线程环境下，则可以直接添加任务，因为在单线程下，该操作一定是线程安全的；但如果是外部线程发起的添加任务，则将此操作分装成一个task，送入EventLoop进行，确保线程安全）
            - 获取系统时间是一个耗时操作，每60次循环进行一次判断，当前时间已经到了分配给非I/O操作的超时时间，退出循环，防止非I/O操作任务过多导致I/O操作被长时间阻塞
    - ByteBuf以及相关辅助类
        - 出现的原因：为了替代java中原生的ByteBuffer实现（长度固定、一旦分配无法动态扩展与收缩、容易发生索引越界异常、只有一个标示位置指针，需要手工处理大量操作）
        - 本质是一个byte数组的缓冲区
        - 使用两个指针（读操作——readerIndex指针，写操作——writeIndex指针；readerIndex <= writeIndex）（双指针算法）
        - 在执行put操作时，会自动的对剩余可用空间进行校验
        - 内存分配
            - 堆内存字节缓冲区：可以被Jvm自动回收，但是存在额外的内存复制开销，需要将堆内存对应的缓冲区复制到内核channel中（消息编解码）
            - 直接字节缓冲区：非堆内存（DirectByteBuf），在堆外进行内存分配，虽然分配以及回收相对慢一些，但是省去了一次内存复制（I/O通信读写缓冲区使用）
        - 零拷贝
            - Netty的接收和发送ByteBuffer采用direct buffer，使用堆外直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。
            - Netty提供了组合Buffer对象（CompositeByteBuf，仅仅是在逻辑上将两个ByteBuffer组合成一个ByteBuffer），可以聚合多个ByteBuffer对象，用户可以像操作一个Buffer那样方便的对组合Buffer进行操作，避免了传统通过内存拷贝的方式将几个小Buffer合并成一个大的Buffer（存在数据拷贝操作）。
            - Netty使用了FileRegion对FileChannel.transfer进行封装，它可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环write方式导致的内存拷贝问题。
            - 对Byte[]数组等等采取的封装操作，其底层的数据存储于byte[]数组一致
        - ByteBuf内存池的实现
            - 重要属性：readIndex（读指针位置）、writeIndex（写指针位置）、capacity（容量）
                - 读指针位置之前的数据为无效数据，[writeIndex - readIndex]为可读的数据，[capacity - writeIndex]为剩余可写的数据大小
            - 分类（继承结构图）
                - ![](https://github.com/chuntaojun/Java_Note/blob/master/web/netty/image/netty-4.png)
                - 内存分配（总共三种形式）
	                - Pooled（每一次从预先申请的内存空间中直接分配） and Unpooled（直接调用底层系统API去申请一块内存空）
                    - Unsafe（直接可以调用JDK的Unsafe进行操作内存空间） and 非Unsafe（底层没有依赖JDK的Unsafe类进行操作），具体的选择情况根据JDK是否可以运行使用Unsafe类来进行选择
                    - Heap（JVM堆内存） and Direct（直接内存）
            - 内存分配器：ByteBufAllocator的作用
                - handle：高32位，低32位
                - 如果是IO的buffer，则更倾向于直接使用DirectBuffer
                - UnpooledByteBufAllocator分析
                    - Heap：直接new一个byte数组，进行内存管理，也就是直接在JVM堆内存中创建一个数组
                    - Direct：使用JDK底层的ByteBuffer进行内存管理操作
                    - Unsafe：直接传入数组对象以及数组下标，通过UNSAFE对象直接操作对象的指针进行偏移查找
			    - PooledByteBufAllocator分析
			        - 在对象创建之初，创建了HeapArenas数组以及DirectArena，提前申请一批内存空间，创建的Arena数组与EventLoop数组个数是一致的，因此确保了每一个线程都分配了一个Arena内存管理对象，因为当线程在申请内存空间时，就避免了资源的竞争（参考redis的内存分配管理：jmalloc）
				    - DirectArena分配内存的流程
					    - 从对象池中获取一个PooledByteBuf对象进行复用
					    - 先从缓存上进行内存分配；如果分配失败，则采用加锁操作，直接调用底层系统调用去申请一块内存空间；分配的空间超过chunSize后，直接申请一块内存空间进行分配
			        - PoolThreadLocalCache：继承自FastThreadLocal，线程局部缓存
				    - 每个线程从PoolThreadLocalCache中取出各自对应的PoolThreadCache对象（内部维护着不同级别的MemoryRegionCache），该对象有一个Arena，Arena下有一个ChunkList组成的双向链表，每个ChunkList是由chunk组成的双向链表；并且会去根据内存的使用率划分不同的ChunkList
			    - Page级别的内存分配：allocatorNormal
				    - 首先尝试在已有的chunk上进行分配；如果失败，则创建一个chunk进行内存分配
			    - SubPage级别的内存分配
				    - 定位一个subpage—>初始化SubPage（找到一个chunkPage，根据SubPage的大小进行等划分）
				    - 通过bitmap标识那个子page被分配
			    - 如何进行回收
				    - 通过对象引用计数器进行判断对象是否可以释放
				    - 连续的内存区段加到缓存中，如果加入失败，则直接标记连续的内存区段为未使用，ByteBuf加入到对象池（减少对象的创建，尽可能的减少GC）
		    - 主要是chunk（16m）以及page（8k）的管理以及组织，chunk被用来组织与管理多个page的内存分配与释放，chunk中的page被构建成一棵二叉树
			    - netty的内存分配管理中采用了两个满二叉树（netty通过数组进行实现满二叉树实现）：memoryMap（存放分配信息）和depthMap（存放节点的高度信息）
                - ![](https://github.com/chuntaojun/Java_Note/blob/master/web/netty/image/netty-5.png)
		    - poolsubpage（0~8k）：page会被切分成大小相等的多个存储块，存储快的大小由第一次申请的内存块大小决定（如果第一次申请的是一个字节，那么这个page只能继续分配一个字节，如果超过一个字节，就需要在新的page中进行分配）
		    - page中的存储区域的使用情况采用一个long数组进行维护
    - Channel
        - 被抽象出来的关于网络I/O的相关操作的接口
	    - 所有channel的读写抽象操作都在AbstractNioUnSafe类中体现
	    - 网络I/O操作会触发ChannelPipeline对应的事件（Netty是基于事件驱动），事件在ChannelPipeline中传播，由对应的ChannelHandler进行拦截处理（可以由事件驱动来实现AOP）
	    - eventLoop：channel需要注册到EventLoop的多路复用器上，用于处理I/O事件
	    - 处理读操作之前，需要对网络的操作位设置为读（doBeginRead()），先要对channel进行判断channel是否关闭
	    - channel的唯一标示生成策略：机器的MAC地址、当前进程的ID、时间戳、随机数
	    - 事件出入站的判断
		    - 入站事件触发，则数据从ChannelPipeline的最左开始，向右传播；
		    - 出站事件触发，则数据从ChannelPipeline的最右开始，向左传播；
		    - 传播事件时，会自动的对Handler做判断，入站事件，如果没有实现ChannelInboundHandler接口，则自动跳过；出站事件，如果没有实现ChannelOutboundHandler接口，则自动跳过
	    - channel过滤
		    - 数据管道抽象为channelpipeline：持有I/O事件拦截器channelhandler的链表；是线程安全的，可以多个线程并发的操作ChannelPipeline，但是channelhandler不是线程安全的；channelpipeline支持哈运行动态的添加或者删除channelhandler
		    - 底层调用read方法读取ByteBuf时，触发channelread读事件
	    - 如何检测一个新连接的接入？新连接是怎样注册到NioEventLoop中
		    - NioEventLoop在单线程中运行select事件，将当前连接进来的客户端装载进List&lt;Object&gt;中，然后通过pipeline的fireChannelRead()方法进行广播accept事件，然后在ServerBootstrapAcceptor中进行对Channel进行分配给不同的EventLoop中，进行selector注册（关心read事件）
                - ![](https://github.com/chuntaojun/Java_Note/blob/master/web/netty/image/netty-6.png)
		    - ServerBootstrapAcceptor
			    - 添加用户的ChannelHandler
			    - 设置options（底层TCP读写相关参数）以及attribute（绑定自定义属性）
			    - 选择NioEventLoop并注册selector
		    - RecvByteBufAllocator的作用：服务端接受的一个速率
		    - Netty在接受客户端连接事件时，会去判断一个selector本次的可读连接数，如果超过最大值则直接跳出循环，结束本次检查新连接接入事件
	    - Channel继承关系图
            - ![](https://github.com/chuntaojun/Java_Note/blob/master/web/netty/image/netty-7.png)
    - 对TCP粘包拆包的解决方案
	    - tcp粘包/拆包发生的原因
		    - 应用程序write写入的字节大于套接口发送缓冲区大小
		    - 进行MSS大小的TCP分段
		    - 以太网帧的playload大于MTU进行IP分片
	    - 拆包基类
		    - ByteToMessageDecoder
		    - 如果发送方发送数据过快如何处理
		    - 如果发送方发送数据过快如何处理
			    起因：如果发送方发送数据过快，可能导致channelReadComplete方法很久才调用到；因此为了防止发送方发送过快，Netty在channelRead方法执行的最后会对字节容器做清理操作（如果当前字节容器没有什么可读了，则直接销毁字节容器；如果连续16次，字节数组中任有数据未读，则对字节容器进行一次数据压缩，将有效的数据整体移动到字节容器首部，避免OOM的发生）
		- 解决方法
	        - LineBasedFrameDecoder：依次遍历ByteBuf中的可读字节，判断换行符，如果存在，则以此位置为结束位置（可配置单行最大长度）
		    - LengthFieldBasedFrameDecoder（https://www.jianshu.com/p/a0a51fd79f62）
		    - 分隔符和定长解码器
			    - 分隔符解码器：DelimiterBasedFrameDecoder：可传入分隔符和单行消息最大长度
			    - 定长解码器：FixedLengthFrameDecoder：直接设置消息长度进行解码
    - Handler的处理
	    - Pipeline（线程安全的，但是ChannelHandler不是线程安全的）
		    - HeadContext、TailContext
		    - Handler与childHandler的区别：handler在初始化时就会执行，而childHandler会在客户端成功connect后才执行
	    - InBound事件以及ChannelInBoundHandler：被动的接收、事件的触发，执行顺序为添加handler的顺序（外部向程序内发送）
	    - OutBound事件以及ChannelOutBoundHandler：向用户主动发起，执行顺序为添加handler的逆序（程序向外传播）
	    - 异常事件的捕获顺序：添加handler的顺序，从哪个handler抛出，就从哪个handler开始向下传播异常
        - 按照输入输出分为两大类ChannelHandler接口：ChannelInboundHandler以及ChannelOutboundHandler
