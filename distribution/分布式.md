1. 分布式理论知识
    - 分布性、对等性、并发性、缺乏全局时钟（控制事件发生的先后顺序）
事务的ACID
        - A：原子性：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即事务不可分割、不可约简
        - C：一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。
        - I：隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。
        - D：持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失
    - 分布式
        - CAP：一个分布式系统不能同时满足一致性、可用性和分区容错性
            - 一致性：多个副本之间是否能够保持一致性的特性（可以追求最终一致性，而不追求强一致性、时间窗口概念，多久之后数据最终一致）
            - 可用性：系统提供的服务必须一直处于可用状态，每一请求操作能够在有限的时间内返回结果
            - 分区容错性：出现故障时仍然要保证对外提供满足一致性和可用性
        - BASE：基本可用、弱状态（允许数据存在中间状态）、AP最终一致性（所有数据副本经过一定时间后达到最终一致性的状态）
            - 其实质是牺牲数据的一致性来满足系统的高可用性
            - 最终一致性的变种
                - 因果一致性（最弱的一致性）：存在因果关系的进程A、B，A更新数据通知B后，B之后对该数据的访问能够获取到A的最新值，B要对该数据更新的话，必须基于A的数据基础之上
                - 读己之所写：进程A更新一个数据项后总能访问到更新过的最新值
                - 会话一致性：在一个会话中实现“读己之所写一致性”
                - 单调读一致性：保证来自同一个进程的写操作被顺序执行
            - 分布式事务
                - X/OpenDTP
                    - 参与事务的角色分为三种：AP（用户程序）、RM（数据库或者消息中间件）、TM（事务管理器、事务协调者、负责接收来自AP发起的XA事务指令，并调度所有参与的RM，确保事务正确执行）
                    - AP触发分布式事务，发送特殊的XA事务指令，这些执行是TM接管，并且发给相关的RM去执行
                    - RM负责执行XA指令，每个RM只负责执行自己的指令
                    - TM负责整个事务过程中的协调工作，检查和验证每个RM的执行情况
                    - 二阶段提交协议
                        - 发起投票表决，通知所有的RM先完成事务提交过程中涉及的各种复杂的准备操作（写redo、undo日志，将耗时操作和准备提前完成，如果准备失败，告知TM）
                        - 真正的提交阶段，TM基于第一个步骤的投票结果进行决策——提交 or 取消，仅当所有参与的RM同意提交时，TM才通知所有的RM正式提交事务
                        - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/52A0202D-A29F-5248-8AC3-2C003286B482.png)
                        - 缺点：性能不高，与2PC的缺点差不多
                    - BASE（eBasy最终一致性方案）
        - 一致性协议与算法
            - 协调者 => 参与者，协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务真正进行提交
            - 两阶段提交（2PC）、只有协调者拥有超时机制、资源层的协议
                - 提交事务请求（事务询问、执行事务、各参与者向协调者反馈事务询问的响应）
                - 类似于参与者对一次事务操作的投票表态过程，各个参与者投票表明是否要继续执行接下去的事务提交操作
                - 执行事务提交
                    - 如果各个参与者反馈为“yes”，执行事务提交
                    - 发送提交请求、事务提交、反馈事务提交结果、完成事务
                    - 如果某个参与者反馈为“no”或者无法接收到所有参与者的反馈响应，就会中断事务（此处是全部的参与者都要等待，调度者必须接受到全部的参与者返回响应才可以执行下一步的指令下发，因此这里存在阻塞问题）
                        - 发送回滚请求、事务回滚、反馈事务回滚结果、中断事务
                - 缺点：同步阻塞、单点问题、脑裂问题
                    - 阻塞状态出现：各个参与者需要等待其他参与者的反馈
                        - 在二阶段提交的执行过程中，所有参与该事务操作的逻辑都处于阻塞状态，各个参与者在等待其他参与者响应的过程中（其实就是等待调度者发布接下去执行的指令信息，但是调度者有需要等待所有的参与者进行反馈），将无法进行其他任务操作
                    - 单点问题：整个系统就一个协调者，一旦协调者出现问题，则整个系统处于崩溃状态
                        - 在二阶段提交的执行出现问题，，其他参与者会一直处于事务资源锁定的状态，则无法处理接下来的所有请求
                    - 数据不一致性（完成步奏一，但是在步奏二出现了错误）：可能导致只有部分参与者收到了commit请求信息（脑裂现象）
                    - 可能无法知道某一事务具体处于什么情况
                        - 如果协调者在发出指令后挂了，然后唯一收到的参与者也挂掉了；在新的leader选举出来后，该事务所处的状态无法得知，无论是回滚还是提交都不合适，因此出现问题
            - 三阶段提交（3PC）资源层的协议
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/219B1286-B897-9C44-96BF-C4580BE93410.png)
                - 将2PC中的提交事务请求一分为二（CanCommit => PreCommit => DoCommit）
                    - Can commit：事务询问、参与者向协调者反馈事务询问的响应（参与者接收到协调者的can commit后，如果自身认为可以顺利执行事务，反馈“yes”，进入预备状态）
                    - Pre commit：执行事务预提交 or 中断事务
                        - 执行事务预提交：发送预提交请求、事务预提交、参与者向协调者反馈事务执行的响应
                        - 中断事务：发送中断请求、中断事务
                    - Do commit：执行提交 or 中断事务
                        - 执行提交：发送提交请求、事务提交、反馈事提交结果、完成事务
                        - 中断事务：发送中断请求、事务回滚、反馈事务回滚结果、中断事务
                    - 优劣
                        - 阶段提交协议最大的优点就是降低了参与者的阻塞范围，并且能够在出现单点故障后继续达成一致
                        - 在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交，因此会造成数据的不一致性情况出现（此问题2PC也存在）
        - TCC协议（服务层的协议）
            - 优秀博客链接：https://huzb.me/2019/06/30/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E2%80%94%E2%80%942PC%E3%80%813PC%E5%92%8CTCC/
            - TCC 将事务的提交过程分为 try-confirm-cancel(实际上 TCC 就是 try、confirm、cancel 的简称) 三个阶段
                - try：完成业务检查、预留业务资源
                - confirm：使用预留的资源执行业务操作（需要保证幂等性）
                - cancel：取消执行业务操作，释放预留的资源（需要保证幂等性）
            - 流程
                - 事务发起方向事务协调器发起事务请求，事务协调器调用所有事务参与者的 try 方法完成资源的预留，这时候并没有真正执行业务，而是为后面具体要执行的业务预留资源，这里完成了一阶段。
                - 如果事务协调器发现有参与者的 try 方法预留资源时候发现资源不够，则调用参与方的 cancel 方法回滚预留的资源，需要注意 cancel 方法需要实现业务幂等，因为有可能调用失败（比如网络原因参与者接受到了请求，但是由于网络原因事务协调器没有接受到回执）会重试。
                - 如果事务协调器发现所有参与者的 try 方法返回都 OK，则事务协调器调用所有参与者的 confirm 方法，不做资源检查，直接进行具体的业务操作。
                - 如果协调器发现所有参与者的 confirm 方法都 OK 了，则分布式事务结束。
                - 如果协调器发现有些参与者的 confirm 方法失败了，或者由于网络原因没有收到回执，则协调器会进行重试。这里如果重试一定次数后还是失败，会怎么样？常见的是做事务补偿。
        - Paxos算法（解决协调者宕机后参与者的资源处于锁定状态后事务无法继续执行、或者出现网络分区导致数据不一致性的出现；因此实现参与者替代协调者，由参与者之间进行协调合作）
            - 特点：每个节点即是参与者，也是协调者
            - 大致原理
                - 被提出的提案中，只有一个会被选定；如果没有提案被提出，则不会有被选定的提案
                - 一个提案被选定后，进程可以获取被选定的提案信息
                - 如果一个进程认为某个提案被选定了，则这个提案必须是真的被选定的那个
                - 系统中存在多个Accpetor（Acceptor集群），Proposer向acceptor集群发送提案，当足够多的acceptor批准这个提案后，则认为该提案被选定了
                    - 一个acceptor必须能够批准不止一个提案
                    - 采用 [ 编号， value] 表示一个提案
                - 阶段一
                    - proposer选择一个提案编号M(n)，向acceptor某个超过半数的子集成员发送编号为M(n)的prepare请求
                    - 如果一个acceptor收到一个编号为M(n)的prepare请求，且编号M(n)大于该acceptor已经响应的所有proposer请求的编号，将它已经批准过的最大编号提案反馈给proposer
                - 阶段二
                    - proposer收到来自半数以上的acceptor对于其发出的编号为M(n)的prepare请求的响应，发送一个针对 [M(n), V(n)] 的提案的accept请求给acceptor
                        - V(n)是收到响应中编号最大的提案的值
                        - acceptor收到针对 [M(n), V(n)] 提案的acceptor请求，只要该acceptor尚未对编号大于M(n)的prepare请求作出响应，就可以通过这个请求
        - Raft协议（被抽象成为一个状态机模型-Replicated State Machines）
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/15B4F4EB-DB58-7643-B9D6-387BF7920ED8.png)
            - 每台服务器通过日志文件（replicated-log）来持久化保存客户端发出的指令序列，供本地状态机顺序执行，只需要保证每台服务器上的日志文件的一致性，即可保证集群里的状态机的一致性
            - 每个节点有三种状态：Follower——>Candidate——>Leader
                - follower：都是被动的, 不会发送任何请求, 只是响应来自leader和candidate的请求
                - candidate：用来选举一个新的leader
                    - 选举超时时间（election timeout），每个follower节点在接收不到Leader节点的心跳消息之后，并不会立即发起新一轮选举，而是需要等待一段时间之后才切换成Candidate状态发起新一轮的选举（这段等待时间就是election timeout）；心跳时间（heartbeat timeout）
                - leader：处理来自客户端的请求, 如果一个客户端与follower进行通信, follower会帮助客户端重定向到leader
                    - 负责处理所有的客户端请求，当接收到客户端的写入请求时，Leader节点会在本地追加一条相应的日志，Follower节点接收到后会对其进行响应，如果集群中过半的节点收到该请求对应的日志记录时，Leader节点认为该日志记录已提交，可以向客户端返回响应
            - 如果处于Follower状态下的节点没有收到关于Leader发送来的信息（HeartBeat包），那么就会自动将自己提升为Candidate状态，同时会发生投票请求给其他节点，申请成为Leader节点；其他节点接受到该请求后，会将自己的投票结果进行返回，这个过程被称为Leader Election（领袖选举）
                - Log Replication：
                    - 特点：连续性，不允许出现日志空洞
                    - 首领节点在收到信息后，自身的数据处于uncommit状态，然后将数据广播给每个Follower节点（first replicates），Follower节点返回响应后（then the leader waits until a majority of nodes have written the entry），Leader节点自身将数据commit之后，向follower节点发送commit请求。
                    - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/A9B7BB18-BCD6-614C-8133-E4FDAC10EDA4.png)
                - Leader Election：每一个节点有一个wait time，在这期间如果没有收到vote request请求的话，则当wait time结束时，节点状态从follower提升为candidate，同时向除了它之外的节点发送 request vote 请求，如果其它节点在本轮中尚未投票，则将为该请求发送自己的 vote 同意；同时发起 request vote 请求的节点自身开启了一个 wait time ，如果在此时间内收到 majority（过半）的节点的同意后，自身从 candidate 状态提升为 leader 状态
                    - Example
                        - 存在A、B、C三个节点，启动时都为follower状态，A节点切换成Cadidate状态并发起选举
                        - 集群中的节点除了记录当前任期号（currentTerm），还会记录在该任期中当前节点的投票结果
                    - 出现无法选出Leader怎么办
                        - 场景：如果有两个或两个以上节点的选举计时器同时过期，则这些节点会 同时由 Follower 状态切换成 Candidate 状态，然后同时触发新一轮选举，在该轮选举中，每个 Candidate节点获取的选票都不到半数，无法选举出 Leader节点
                        - 由于election timeout是在一个时间区间内的随机数，多次出现无法选出Leader的情况出现的概率不大
                - HeartBeat：心跳机制，如果follower超过一定时间没有收到heartbeat信息，则会认为leader已down了，将自己从follower状态提升为candidate状态
                    - 当存在两个candidate节点同时向其他节点发起 request vote 请求时，如果得到的 vote 没有达到 majority，则会开启新一轮的选举
                        - 问题：存在均票的结果怎么办：Raft每个节点会在wait-time上随机加上一个等待时间，避免此情况的出现，即使出现了，由于随机时间的存在，下一次在出现的概率很小
                        - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/70F8DDFC-8633-CD43-A9AC-A7A6D0BC45F1.png)
            - Log entries只能从Leader发送给其他服务器，事实上Follower不主动发送，而只响应来自Leader和Candidate的请求。客户端只能和Leader交互，如果客户端首先连上了Follower，那么会被Follower转发给Leader
            - 日志复制
                - 对于日志的更改操作，采用Append Entries的形式，并且通过AppendEntries RPC请求去把日志信息广播给Follower
                - 步骤
                    - Client端向Leader发送一条数据，Leader将数据记录后，准备在下一次的AppendEntries RPC中将信息广播给所有的Follower
                    - 信息被超过半数的Follower同意后，Leader准备进行Commit，同时对Client进行响应
                    - 然后在HeartBeat中向Follower发起Commit指令
                - 网络分区的情况下
                    - Client向旧的Leader发起数据，但是由于Majority机制的存在，log无法被Commit
                    - 网络分区恢复之后，由于Term的存在，原有的Leader以及Follower自动跟随新的Leader
            - Raft日志机制的特性
                - 如果在不同的日志中的两个条目有着相同的索引和任期号, 那么他们存储的命令肯定是相同的
                    - 源于leader在一个任期里给定的一个日志索引最多创建一条日志条目, 同时该条目在日志中的位置也从不会改变
                - 如果在不同的日志中的两个条目有着相同的索引和任期号, 那么他们之前的所有日志条目都是完全一样的
                    - 源于AppendEntries RPC的一个简单的一致性检查: 当发送一个AppendEntries RPC时leader会把新日志之前的一个日志条目的索引位置和任期号都包含在里面, follower会检查是否与自己的日志中的索引和任期号是否匹配, 如果不匹配就会拒绝这个日志条目, 接下来就是归纳法来证明了
                - leader通过强制follower复制它的日志来处理日志的不一致
                    - 为了是follower的日志同自己的一致, leader需要找到follower与它日志一致的索引位置并让follower删除该位置之后的条目, 然后再将自己在该索引位置之后的条目发送给follower, 这些操作都在AppendEntries RPC进行一致性检查时完成
                    - leader为每一个follower维护了一个nextIndex, 用来表示将要发送给该follower的下一条日志条目索引, 当一个leader开始掌权时, 会将nextIndex初始化为它的最新日志条目索引值+1, 如果follower在一致性检查过程中发现自己的日志和leader不一致, 会在这个AppendEntries RPC请求中返回失败, leader收到响应之后会将nextIndex递减然后重试, 最终nextIndex会达到一个leader和follower日志一致的位置, 这个时候AppendEntries RPC会成功, follower中冲突的日志条目也被移除了, 此时follower和leader的日志就一致了
                    - 优化方案：假如Follower拒绝了的Leader的AppendEntries的请求，那么可以直接寻找该任期内所有日志条目索引的最小值的那个，反馈给Leader，那么Leader就可以根据Follower的反馈跳跃式的递减nextIndex值，跨过那个任期内的所有冲突的日志
                - 如何确保每一个状态机都按照相同的顺序执行执行同样的指令
                    - 没有包含所有已提交日志条目的节点成为不了领导人（term比对，永远只选择大的）
                    - 日志条目只有一个流向——从Leader流向Follower，领导人永远不会覆盖已经存在的日志条目
                    - 如果被调用方的日志比RPC调用方的来的新，那么就会拒绝候选人投票的请求（对条件一的再次佐证）
                    - Leader在当前任期至少有一条日志被提交，即被超过半数的节点写盘
            - 集群成员变化
                - Raft采用联合一致性的方式来解决节点变更, 先提交一个包含新老节点结合的Configuration, 当这条消息commit之后再提交一个只包含新节点的Configuration, 具体流程如下
                    - 首先对新节点进行CaughtUp追数据
                    - 全部新节点完成CaughtUp之后, 向新老集合发送Cold+new命令
                    - 如果新节点集合多数和老节点集合多数都应答了Cold+new, 就向新老节点集合发送Cnew命令
                    - 如果新节点集合多数应答了Cnew, 完成节点切换
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/F6F2507A-6C12-2745-87AB-1C580241AC7B.png)
            - 蚂蚁金服 SOFA-JRAFT的实现特点
                - 一致性线性读的实现原理
                    - 常规一致性线性读的原理：Raft log read
                        - 每次的读操作都走一次raft协议将读请求同样按照 Log 处理，通过 Log 复制和状态机执行来获取读结果，然后再把读取的结果返回给 Client
                        - 缺点：每次 Read 都需要走 Raft 流程，Raft Log 存储、复制带来刷盘开销、存储开销、网络开销，走 Raft Log不仅仅有日志落盘的开销，还有日志复制的网络开销，另外还有一堆的 Raft “读日志” 造成的磁盘占用开销，导致 Read 操作性能是非常低效的
                    - 优化方案一：ReadIndex Read
                        - 当 Leader 需要处理 Read 请求时，Leader 与过半机器交换心跳信息确定自己仍然是 Leader 后可提供线性一致读：
                            - Leader 将自己当前 Log 的 commitIndex 记录到一个 Local 变量 ReadIndex 里面；
                            - 接着向 Followers 节点发起一轮 Heartbeat，如果半数以上节点返回对应的 Heartbeat Response，那么 Leader就能够确定现在自己仍然是 Leader；
                            - Leader 等待自己的 StateMachine 状态机执行，至少应用到 ReadIndex 记录的 Log，直到 applyIndex 超过 ReadIndex，这样就能够安全提供 Linearizable Read，也不必管读的时刻是否 Leader 已飘走；
                            - Leader 执行 Read 请求，将结果返回给 Client。
                        - 使用 ReadIndex Read 提供 Follower Read 的功能，很容易在 Followers 节点上面提供线性一致读，Follower 收到 Read 请求之后：
                            - Follower 节点向 Leader 请求最新的 ReadIndex；
                            - Leader 仍然走一遍之前的流程，执行上面前 3 步的过程(确定自己真的是 Leader)，并且返回 ReadIndex 给 Follower；
                            - Follower 等待当前的状态机的 applyIndex 超过 ReadIndex；
                            - Follower 执行 Read 请求，将结果返回给 Client。
                            - 缺点：由于使用了RPC的调用，存在一定的网络开销，因此理论性能为RPC的80%
                    - 优化方案二：Lease Read
                        - 通过 Clock + Heartbeat 的 Lease Read 优化方法，也就是 Leader 发送 Heartbeat 的时候首先记录一个时间点 Start，当系统大部分节点都回复 Heartbeat Response，由于 Raft 的选举机制，Follower 会在 Election Timeout 的时间之后才重新发生选举，下一个 Leader 选举出来的时间保证大于 Start+Election Timeout/Clock Drift Bound，所以可以认为 Leader 的 Lease 有效期可以到 Start+Election Timeout/Clock Drift Bound 时间点。Lease Read 与 ReadIndex 类似但更进一步优化，不仅节省 Log，而且省掉网络交互，大幅提升读的吞吐量并且能够显著降低延时。
                        - 基本思路是 Leader 取一个比 Election Timeout 小的租期（最好小一个数量级），在租约期内不会发生选举，确保 Leader 不会变化，所以跳过 ReadIndex 的第二步也就降低延时。由此可见 Lease Read 的正确性和时间是挂钩的，依赖本地时钟的准确性，因此虽然采用 Lease Read 做法非常高效，但是仍然面临风险问题，也就是存在预设的前提即各个服务器的 CPU Clock 的时间是准的，即使有误差，也会在一个非常小的 Bound 范围里面，时间的实现至关重要，如果时钟漂移严重，各个服务器之间 Clock 走的频率不一样，这套 Lease 机制可能出问题
                        - 实现方式
                            - 定时 Heartbeat 获得多数派响应，确认 Leader 的有效性
                            - 在租约有效时间内，可以认为当前 Leader 是 Raft Group 内的唯一有效 Leader，可忽略 ReadIndex 中的 Heartbeat 确认步骤
                            - Leader 等待自己的状态机执行，直到 applyIndex 超过 ReadIndex，这样就能够安全的提供 Linearizable Read
            - Multi-Raft-Group
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/42EA6365-FCB8-BC41-B55F-E7E07F9042A0.png)
                - 原因：用户在对一组 Raft 系统进行更新操作时必须先经过 Leader，再由 Leader 同步给大多数 Follower。而在实际运用中，一组 Raft 的 Leader 往往存在单点的流量瓶颈，流量高便无法承载，同时每个节点都是全量数据，所以会受到节点的存储限制而导致容量瓶颈，无法扩展；MULTI-RAFT-GROUP 正是通过把整个数据从横向做切分，分为多个 Region 来解决磁盘瓶颈，然后每个 Region 都对应有独立的 Leader 和一个或多个 Follower 的 Raft 组进行横向扩展，此时系统便有多个写入的节点，从而分担写入压力
            - Rheakv
                - 原理图
                    - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/3A051757-5226-474D-85E8-B7D2E0511786.png)

2. Dubbo微服务框架
    - 系统拆成分布式的原因
        - 服务从一个拆分成多个，单体服务的代码量减少，同时每个服务各自的环境依赖不在相互影响
        - 技术版本可能存在冲突，但是某个功能模块又需要用最新的技术版本依赖
    - 工作原理
        - service层：接口层，给服务提供者与消费者实现
        - config层：配置层，用于dubbo的配置
        - proxy层：服务代理层，透明生成客户端的stub以及服务端的skeleton
        - registry层：服务注册层，负责服务的注册以及发现
        - cluster层：集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务
        - monitor层：监控层，对rpc接口的调用次数和调用时间进行监控
        - protocol层：远程调用层，封装rpc请求
        - exchange层：信息交换层，封装请求响应模式，同步转异步
        - transport层：网络传输层，抽象mina和netty为统一接口
        - serialize层：数据序列化层
        - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/81036F38-72B2-0149-83F1-AD85197ABF59.png)
    - 服务调用链路
    - 服务降级
    - 负载均衡
        - Dubbo的几种负载均衡算法
            - RandomLoadBalance：RandomLoadBalance 是加权随机算法的具体实现，它的算法思想很简单。假设我们有一组服务器 servers = [A, B, C]，他们对应的权重为 weights = [5, 3, 2]，权重总和为10。现在把这些权重值平铺在一维坐标值上，[0, 5) 区间属于服务器 A，[5, 8) 区间属于服务器 B，[8, 10) 区间属于服务器 C。接下来通过随机数生成器生成一个范围在 [0, 10) 之间的随机数，然后计算这个随机数会落到哪个区间上就选择该区间对应的服务。比如数字3会落到服务器 A 对应的区间上，此时返回服务器 A 即可。权重越大的机器，在坐标轴上对应的区间范围就越大，因此随机数生成器生成的数字就会有更大的概率落到此区间内。只要随机数生成器产生的随机数分布性很好，在经过多次选择后，每个服务器被选中的次数比例接近其权重比例。
            - LeastActiveLoadBalance：活跃调用数越小，表明该服务提供者效率越高，单位时间内可处理更多的请求。此时应优先将请求分配给该服务提供者。在具体实现中，每个服务提供者对应一个活跃数 active。初始情况下，所有服务提供者活跃数均为0。每收到一个请求，活跃数加1，完成请求后则将活跃数减1。在服务运行一段时间后，性能好的服务提供者处理请求的速度更快，因此活跃数下降的也越快，此时这样的服务提供者能够优先获取到新的服务请求、这就是最小活跃数负载均衡算法的基本思想。除了最小活跃数，LeastActiveLoadBalance 在实现上还引入了权重值。所以准确的来说，LeastActiveLoadBalance 是基于加权最小活跃数算法实现的
            - RoundRobinLoadBalance：所谓轮询是指将请求轮流分配给每台服务器。举个例子，我们有三台服务器 A、B、C。我们将第一个请求分配给服务器 A，第二个请求分配给服务器 B，第三个请求分配给服务器 C，第四个请求再次分配给服务器 A。这个过程就叫做轮询。轮询是一种无状态负载均衡算法，实现简单，适用于每台服务器性能相近的场景下。但现实情况下，我们并不能保证每台服务器性能均相近。如果我们将等量的请求分配给性能较差的服务器，这显然是不合理的。因此，这个时候我们需要对轮询过程进行加权，以调控每台服务器的负载。经过加权后，每台服务器能够得到的请求数比例，接近或等于他们的权重比。比如服务器 A、B、C 权重比为 5:2:1。那么在8次请求中，服务器 A 将收到其中的5次请求，服务器 B 会收到其中的2次请求，服务器 C 则收到其中的1次请求
        - 如何实现负载均衡的
    - 接口调用顺序性
        - 内存MQ，请求带上order-id（请求顺序入队，请求进行路由操作）
        - 分布式锁（重量级锁，性能会下降）
    - 幂等性的保证（对于业务的理解，不是技术难问题）
        - 每个请求一个唯一的ID标识
        - 每次处理完一个请求，需要对他进行标记操作（发起支付前先发送一个支付流水、比如数据库记录一个状态 or redis使用set数据结构）
        - 对请求需要进行判断是否已被处理
    - SPI的自适应机制
        - 某些拓展希望被延迟加载，因此存在一个矛盾：拓展未加载——拓展无法被调用——拓展无法被加载
        - 为接口创建一个代理类，在代理类进行拓展类延迟加载（方法调用时才加载，如果拓展被加载后会被缓存起来）
            - Dubbo 会为拓展接口生成具有代理功能的代码。然后通过 javassist 或 jdk 编译这段代码，得到 Class 类。最后再通过反射创建代理类
            - 对于存在创建自适应拓展类的接口要求至少有一个方法被@Adaptive所注解，否则将抛出运行时异常
            - Dubbo 不会为没有标注 Adaptive 注解的方法生成代理逻辑，对于该种类型的方法，仅会生成一句抛出异常的代码
    - 服务导出事件
        - 基于 spring的容器刷新事件（spring内部的消息编程、事件监听机制），Dubbo在接收到事件通知时，将执行服务导出逻辑
        - 服务提供者以及服务消费者对于方法的调用过程中都会出现Invoker，它是Dubbo的核心模型，代表着一个可执行的实体（既可以代表本地服务实现，也可以代表着远程实现）；默认创建方式为JavassistProxy代理方式创建
    - Dubbo客户端实现的连接复用、隐式传参、泛化调用
    - 心跳机制的实现

3. gRPC
 

4. 延时消息队列
    - 如何实现一个支持任意延迟时间的延时消息队列：https://www.cnblogs.com/hzmark/p/mq-delay-msg.html
    - 关键数据结构：TimeWheel（Netty中广泛使用）
        - 一个环形数组+时间指针，时间指针按照固定的时间移动，每次的移动成为一个tick（每一次移动数组的一个单元）
        - 根据公式：假设这个延迟时间为X秒，那么X%(ticksPerWheel * tickDuration)可以计算出X所属的TimeWheel中位置
            - 参数含义：ticksPerWheel（一轮的tick数），tickDuration（一个tick的持续时间）
        - 优化
            - 采用多阶级的TimeWheel，每个TimeWheel对应不同的时间级别（时、分、秒），可以避免：（缺点）如果先发送一个需要九秒延迟的消息，后又发送一个需要一秒延迟的消息，但是由于只有一个TimeWheel，会导致一秒的延迟消息需要等待九秒的延迟消息处理完后才能被处理到
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/C66BC42C-D03C-5347-AC4E-528605680D35.png)
            - 数据加载优化：只加载时间临近的数据集，离目标时间较久的数据集不予加载，因此一定程度上减少了需要加载进内存的数据量

5. Kafka
    - 架构图
        - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/1403FAC7-3D33-A64C-98B8-20F5C3B13F00.png)
    - kafka获取消息的模式
        - 采取pull模式（客户端主动拉取模式）
            - push模式容易忽略消费者的消息处理能力，可能导致消息不断在消费者端堆积，最终将消费者压垮
            - pull模式是根据消费者自身的消费能力向生产者拉取消息，因此对消费者友好，但是很容易出现空轮询问题（生产者对应的主题中没有相应的消息，因此消费者会不断的轮询生产者，结果什么都没有拿到，浪费性能；或者当消费者无法工作时，导致大量的消息挤压在消息队列中）
            - 附录
                - 现在大多数都是采取推拉结合的模式，首次采取拉，后面采用推的模式
    - 分布式提交日志，数据按照一定顺序持久化保存
        - 索引
            - 偏移量索引文件用来建立消息偏移量到物理地址之间的映射关系，快速定位消息所在的物理文件位置；时间戳索引文件根据指定的时间戳来查找对应的偏移量信息
            - 索引文件以稀疏索引的方式构造消息的索引，不保证每个消息在索引文件中都有对应的索引项；稀疏索引通过MappedByteBuffer将索引文件映射到内存中；由于偏移量是单调递增的，查询指定偏移量时采用二分法定位
            - 偏移量索引分为两个部分：relativeOffset（相对偏移量）以及position（物理量地址）
        - 日志清理
            - 每一个分区副本对应一个log，而log又可以分为多个日志分段；kafka提供两种日志清理策略：日志删除（按照一定保留策略直接删除不符合条件的日志分段）以及日志压缩（对消息的key进行整合，只保留最后一个value）
            - 存在专门的日志删除任务来周期性检测和删除不符合保留条件的日志分段文件（基于时间、基于日志大小以及基于日志起始偏移量）
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/46F2BFFD-5A49-E540-B4AF-359959CF8F87.png)
    - 消息被分批次写入kafka
    - 主题与分区
        - 消息通过主题进行分类（主题好比数据库的表）
        - 主题可以被分为若干个分区，一个分区就是一个提交日志，消息以追加的形式写入分区，然后以先入先出的顺序读取（由于一个主题存在多个分区，无法保证整个主题下的消息的有序性，但是可以保证单个分区内的消息有序性）
            - 可通过消息键+分区器（分区器为键生成一个散列值，将其映射到指定的分区上）实现消息写入指定分区
        - 消费者订阅一个或多个主题，按照消息生成的顺序读取，通过检查消息的偏移量（每个消息的偏移量是唯一的）来区分已经读过的消息；存在多个消费者共同读取一个主题，消费者群组保证每个分区只能被消费者群组中的其中一个消费者使用，如果某个消费者失效，群组中的其他消费者会接管工作
        - 对于Kafkafe分区中而言，它的每条消息都有唯一的offset，用来表示消息在分区中对应的位置
        - 为什么主题要设置多个分区、为什么日志需要进行分段
            - 主题分区的原因：可以进行水平的扩展
            - 日志分段的原因：如果不进行分段操作，则日志大小会随着运行不断增大，同时导致查询速度减慢
    - Broker
        - 每个broker有一个唯一的标识符，broker启动时创建临时节点把自己的ID注册到Zookeeper
        - 一个独立的Kafka服务器被称为 broker，是集群的组成部分，每个集群中有一个broker充当集群控制器的角色
        - 一个分区从属于一个broker，该broker被称为分区首领；分区可以分配给多个broker，这时候会发生分区复制（产生了消息冗余）
        - 第一个启动的broker会在Zookeeper里创建一个临时节点 /controller让自己成为控制器，其他节点在创建 /controller 会收到节点已存在的通知，则会在控制节点上创建Zookeeper watch对象，接收该节点的变更通知；从而确保集群中只有一个控制器存在；每个新选出的控制器节点会根据Zookeeper的条件递增获取一个全新的，数值更大的controller epoch（用于忽略旧的epoch消息、避免脑裂现象的发生：有两个节点认为自己是当前的控制器）
    - 复制
        - 副本（利用副本，kafka使用了多副本机制实现故障的自动转移，保证了当kafka集群中某个broker失效时服务可用）
            - 每个分区有多个副本，副本被保存在broker上，分区中的副本统称AR：Assigned Replicas
            - 首领副本：每个分区都有一个首领副本，所有生产者请求和消费者请求都会经过这个副本；搞清楚哪个跟随者的状态是和自己一致的
            - 跟随者副本：首领以外的副本都是跟随者副本，跟随者副本不处理来自客户端的请求，只从首领副本复制消息（同步期内follower副本相对于leader副本还是会有一定程度的滞后：网络因素），保持与首领副本一致的状态，如果首领发生崩溃，其中一个跟随者会被提升为新的首领；跟随者副本发送了三个请求消息，在收到这三个请求消息之前，跟随者是不会在发送第四个请求消息的，如果发送了，那么首领就知道跟随者已经收到了前面三个请求的响应消息
            - 持续请求得到最新消息副本的被称为同步副本（ISR：In-Sync Replicas），在首领失效时，只有同步副本才有可能被选为新首领
            - 副本机制架构图
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/CF8A9215-6B59-A048-AFE8-8019FF336037.png)
    - 处理请求
        - broker会在它监听的每一个端口运行一个Acceptor线程，由它创建的链接会分发至processor线程去处理（类似于EventLoop）
        - 生产请求与获取请求都必须发送给分区的首领副本（客户端要自己负责把生产请求与获取请求发送到正确的broker上）
            - 配置部分broker节点地址后，客户端会自行发现其他broker节点的地址信息（nacos也有实现一样的功能）
            - 客户端采用发送元数据请求（包含了客户端感兴趣的主题列表，服务器端的响应指明了这些主题所包含的分区、分区中有哪些副本以及首领副本），元数据请求可以发送给任意一个broker（为什么可以发送给任意一个broker），客户端会把这些信息缓存起来
            - 客户端需要时不时通过发送元数据请求刷新信息（刷新间隔可配置），从而知道元数据是否发生了变更
        - 生产请求（Producer）
            - 验证发送数据的用户是否有主题写入权限、acks值是否有效、acks=all时是否有足够多的同步副本保证消息已经被安全写入
            - acks参数
                - 指定了需要多少个broker确认才能认为一个消息写入是成功的
                    - Acks=0：生产者把消息发送出去之后，不需要等待broker响应
                    - Acks=1：只要首领收到消息就认为写入成功
                    - Acks=all：只有所有完全同步副本收到消息才算写入成功（能够确保消息一定不会丢）
            - 消息写入本地磁盘，在linux系统上，消息会被写入文件系统缓存；kafka不会一直等待数据被写到磁盘，依赖复制功能保证消息的持久性
            - 采用零复制技术：kafka直接把消息从文件（linux文件系统缓存）发送到网络通道，而不需要经过任何中间缓冲区
                - 通常文件到套接字的数据传输路径
                    - 操作系统从磁盘读取数据到内核空间pagecache
                    - 应用程序读取内核空间的数据到用户空间的缓冲区
                    - 应用程序将数据（用户空间的缓冲区）写回内核空间到套接字缓冲区（内核空间）
                    - 操作系统将数据从套接字缓冲区（内核空间）复制到网络发送的NIC缓冲区
            - 一个topic对应多个partition，而kafka能够确保一个partition内的消息是有顺序的，并且生产者在写的时候，可以指定一个key，那么相关的数据会被路由到一个partition中
                - 路由方式
                    - 指定了partition
                    - 未指定partition，但是指定了key，通过对key的value进行hash选出一个partition
                    - partition和key均未指定，如果 key 不为 null，那么计算得到的分区号会是所有分区中的任意一个；如果 key 为 null 并且有可用分区时，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别
            - 架构情况
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/C8E92573-5B7D-DB4D-95D1-5B31D92BE394.png)
        - 获取请求（Consumer）
            - 消费者与消费组（每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者，每个消费者只能消费所分配到的分区中的消息。换言之，每一个分区只能被消费组中的一个消费者所消费）
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/2DBDBA57-FC23-D143-BE8B-66842FE9F743.png)
            - 能够指定broker最多从一个分区里返回多少数据，也能够设置下限（告诉broker等到有多少数据后再发送，可以设置一个超时时间，避免无限期等待）
            - 消费端具备一定的容灾能力：使用pull模式拉取消息，并且保存消费的具体位置，当消费者宕机后再次上线时可以根据offset接着进行消息消费
            - 能够对消息消费速度进行控制，通过pause()以及resume()方法实现暂停某些分区在拉取操作时返回数据给客户端以及恢复某些分区向客户端返回数据的操作
            - 消息投递模式的自由转换
                - 点对点模式：基于队列的，消息生产者发生消息到队列，消息消费者从队列中接收消息
                - 发布订阅模式：消息发布者将消息发布到某个主题，而消息订阅者从主题中订阅消息，主题使得消息的订阅者与发布者保持相互独立
                - 自由切换消息投递模式的原理：kafka的消息投递模式的自由转换：如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用；如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布/订阅模式的应用
            - 大部分客户端只能读取已经被写入同步副本的消息
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/0892B27C-AFFB-D842-A3E6-F7E9621C3360.png)
        - 物理存储
            - 使用log.dirs设置存储分区的目录清单
            - 分区分配
                - 创建一个包含十个分区的主题，复制系数为3，则会创建30个分区副本
                - 确保分区的每个副本分布在不同的broker上
                - 新的分区总是被添加到数量最小的那个目录里
            - 文件
                - 正在写入数据的片段叫做活跃片段，活跃片段永远不会被删除
                - 文件格式
                    - 键、值、偏移量、消息大小、校验和、消息格式版本号、压缩算法（Snappy、GZip、LZ4）和时间戳
                    - 如果生产者发送的是压缩过的消息，那么同一批次的消息会被压缩在一起  
                     - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/CBB64EAE-8CC2-6442-A43F-41C5A4035AA1.png)
                - 索引：为每个分区维护了一个索引，将偏移量映射到片段文件和偏移量在文件里的位置；索引也被分成片段；kafka不维护索引的校验和，如果索引出现损坏，kafka会通过重新读取消息并录制偏移量和位置重新生成索引
        - 可靠的数据传递
            - 保证分区消息的顺序性；只有当消息被写入分区的所有同步副本时，才会被认为是“已提交的”；只要还有一个副本是活跃的，已经提交的消息就不会丢失；消费者只能读取已经提交的消息
            - 复制机制
                - 跟随者可被认为是同步的条件：与zookeeper之间有活跃的对话、过去规定时间内从首领获取过消息、
                - 复制系数：复制系数为N的话，在N-1个broker失效的情况下，仍能够从主题读取数据或向主题写入数据；但同时也会占用N倍的存储空间
    - 内存管理
        - 消息在网络中都是以字节的形式进行传输的，在发送之前需要创建一块内存区域来保存对应的消息，在 Kafka 生产者客户端中，通过 java.io.ByteBuffer 实现消息内存的创建和释放。不过频繁的创建和释放是比较耗费资源的，在 RecordAccumulator 的内部还有一个 BufferPool，它主要用来实现 ByteBuffer 的复用，以实现缓存的高效利用。不过 BufferPool 只针对特定大小的 ByteBuffer 进行管理，而其他大小的 ByteBuffer 不会缓存进 BufferPool 中

6. Zookeeper（CP模型）
    - 启动流程
        - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/DDFA36ED-B3C8-3747-B26C-C2E2FB8D1401.png)
    - 所解决的问题
        - 半数以下节点宕机，依旧可以对外工作
        - 客户端的所有写请求，均交由Leader进行处理，写入成功后，需要同步给所有的follower以及observer
        - Leader宕机，或者集群重启。需要确保已经在Leader提交的事务最终都能被服务器提交，并且确保集群能快速恢复到故障前的状态
    - 基本概念
        - 数据节点（dataNode）：zk 数据模型中的最小数据单元，数据模型是一棵树，由斜杠（ / ）分割的路径名唯一标识，数据节点可以存储数据内容及一系列属性信息，同时还可以挂载子节点，构成一个层次化的命名空间
        - 事务及 zxid：事务是指能够改变 Zookeeper 服务器状态的操作，一般包括数据节点的创建与删除、数据节点内容更新和客户端会话创建与失效等操作。对于每个事务请求，zk 都会为其分配一个全局唯一的事务 ID，即 ZXID，是一个 64 位的数字，高 32 位表示该事务发生的集群选举周期（集群每发生一次 leader 选举，值加 1），低 32 位表示该事务在当前选择周期内的递增次序（leader 每处理一个事务请求，值加 1，发生一次 leader 选择，低 32 位要清 0）
        - 事务日志：所有事务操作都是需要记录到日志文件中的，可通过 dataLogDir 配置文件目录，文件是以写入的第一条事务 zxid 为后缀，方便后续的定位查找。zk 会采取“磁盘空间预分配”的策略，来避免磁盘 Seek 频率（文件的不断追加写入操作会触发底层磁盘I/O为文件开辟新的磁盘块），提升 zk 服务器对事务请求的影响能力。默认设置下，每次事务日志写入操作都会实时刷入磁盘，也可以设置成非实时（写到内存文件流，定时批量写入磁盘），但那样断电时会带来丢失数据的风险
        - 日志截断：Zookeeper的原则是只要集群中存在Leader那么所有的机器必须保持与Leader数据同步，因此，当发现存在数据不同步时，会下发指令给对应的follower，要求进行日志截断与删除
        - 事务快照：数据快照是 zk 数据存储中另一个非常核心的运行机制。数据快照用来记录 zk 服务器上某一时刻的全量内存数据内容，并将其写入到指定的磁盘文件中，可通过 dataDir 配置文件目录。可配置参数 snapCount，设置两次快照之间的事务操作个数，zk 节点记录完事务日志时，会统计判断是否需要做数据快照（距离上次快照，事务操作次数等于snapCount/2~snapCount 中的某个值时，会触发快照生成操作，随机值是为了避免所有节点同时生成快照，导致集群响应缓慢）。进行现场恢复的日志文件，但是又不能光光依靠快照文件进行现场恢复，因为快照文件创建存在时间间隔（事务粗粒度太大），可能导致数据丢失
    - 节点状态
        - LOOKING：节点正处于选主状态，不对外提供服务，直至选主结束
        - FOLLOWING：作为系统的从节点，接受主节点的更新并写入本地日志
        - LEADING：作为系统主节点，接受客户端更新，写入本地日志并复制到从节点
    - 同步方式
        - follower 中写入日志但是未提交的 zxid 。称之为服务器提议缓存队列 committedLog 中的 zxid
        - 三种ZXID：peerLastZxid（该learner（observer、follower）服务器最后处理的zxid）、minCommittedLog（leader服务器提议缓存队列 committedLog 中的最小 zxid ）、maxCommittedLog（leader服务器提议缓存队列 committedLog 中的最大 zxid）
        - 直接差异化同步（peerLastZxid介于minCommittedLogZxid和maxCommittedLogZxid间）
            - leader发出了同步请求，但是还没有Commit就down了，因此新选出的leader会继续完成上一任leader未完成的工作
        - 先回滚再差异化同步/仅同步回滚
            - leader写入本地事务日志后，还没发送同步指令就down了，然后在日志同步时作为learner出现；因此未发出同步请求的日志信息直接丢弃
            - 新的leader如果没有处理新的请求，那么leaner仅仅需要回滚就好了，如果新的leader处理了新的请求，那么leaner需要先进行回滚操作，然后在和leader同步新的日志信息
        - 全量同步
            - peerLastZxid小于minCommittedLogZxid或者leader上面没有缓存队列。leader直接使用SNAP命令进行全量同步
    - Znode
        - 存在生命周期（取决于节点的类型）
            - 持久节点（节点创建后一直存在，直到有对该节点的删除操作）
                - 典型场景：集群的配置，结合watch实现集群的配置实时生效
            - 临时节点（生命周期与创建这个节点的客户端会话绑定）
                - 可用于感知集群中的那些机器是可用的
                - 可用于服务发现和服务路由
            - 时序节点（创建子节点时可以设置这个属性，节点名后面跟上一个数字后缀，数字后缀的最大值为整形的最大值）
                - 可以简单的用做master选举机
            - 临时性时序节点（主要用于分布式锁的实现）
        - 集群控制子系统的原理架构图
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/F3E90E0C-3320-724E-9DA7-F8A6FAD8EC4C.png)
    - 保证顺序一致性、原子性（所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，即整个集群所有机器要么都成功应用了某一个事务，要么都没应用）、单一视图（无论链接的是哪一个zookeeper服务器，看到的服务端数据模型都是一致的）、可靠性、实时性
    - 通过共享的、树型结构的名字空间进行相互协调（ZNode数据节点，将全量数据存储在内存中，每个znode都会存储数据）
    - 集群模式
        - 每台机器都会在内存中维护当前的服务器状态，每台服务器之间都互相保持通信，只要集群中存在超过一半的机器能够正常工作就可以对外服务
        - zookeeper会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序（可以用来实现更高层次的同步原语）
    - leader、follower、observer三种角色，通过leader选举选定一台被称为leader的机器，leader服务器为客户端提供读和写服务（follower、observer也可以提供读服务）；observer机器不参与leader选举过程，也不参与写操作的过半写成功
    - 客户端通过心跳检测与服务器保持有效的会话、向zookeeper服务器发送请求、接收来自服务器的watch事件通知
    - ZAB协议
        - 支持崩溃恢复的原子广播协议，并不通用，专门为zookeeper设计的
        - 使用单一的主进程接收并处理客户端的所有事务请求，将服务器数据的状态变更以事务proposal的形式广播到所有的副本进程；保证了同一时刻集群中只能够有一个主进程来广播服务器的状态变更，保证全局的变更序列被顺序应用
        - 两种基本模式（崩溃恢复、消息广播）
            - 崩溃恢复：选举产生新的leader，当选举出新的leader服务器，并且集群中已有过半的机器与该leader服务器完成同步之后，退出恢复模式
                - 确保最新选举出来的leader服务器拥有集群中所有机器的最高编号（ZXID最大）的事务proposal，可以保证新选举出来的leader一定具有所有已经提交的提案
                - 准leader需要各个进程之间的数据同步之后，准leader进程才能真正成为新的主进程周期中的leader
                - 一个follower只能和一个leader保持同步（通过心跳检测机制感知彼此的情况）
            - 消息广播：非leader服务器会把接收的客户端请求转发给leader服务器
                - 二阶段提交过程，leader服务器接收到客户端请求，生成相应的事务proposal，发送给集群中的其余所有机器（follower），在分别收集各自的选票，然后进行事务提交
                    - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/BD90E3E6-1BFA-5F40-9B98-A0AB0AF1544F.png)
                - 移除了中断逻辑，使得只需要过半的follower响应了leader服务器就可以开始提交事务了（会为每个proposal分配一个全局的单调递增的ID）
                - 整个消息的广播是基于具有FIFO特性的TCP协议来进行网络通信的，leader服务器会为每一个follower服务器都各自分配一个单独的队列，将需要广播的事务proposal放到队列中，根据FIFO的策略进行消息发送；follower接收到proposal后将其以事务日志形式写入本地磁盘中去，写入成功后反馈一个ACK至leader服务器，接收到过半的ACK后，通知follower进行事务提交
                    - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/0C42D9A1-D382-A245-890D-E0CF731D9525.png)
            - 与Paxos的联系与区别
                - 均存在一个类似于leader进程的角色，负载协调多个follower进程的运行
                - leader进程都会等待过半的follower作出正确的反馈后才会将一个提案进行提交
                - 每个事务包含了一个epoch值，用来代表当前的leader周期
    - 使用ACL访问权限列表，决定当前操作API
    - 应用场景
        - 配置通知管理：将配置文件内容存储至zookeeper中，客户端订阅一个watcher，一旦配置更改，zookeeper会发送通知事件到各个客户端，客户端在接收后自行对数据进行拉取最新的数据

7. 负载均衡
    - 服务预热
        - 为了避免服务启动之初就处于高负债的状态，服务预热是一个优化手段，与此类似的还有JVM预热，主要目的是让服务启动后低功率运行一段时间，使其效率慢慢提升至最佳状态
    - 一台服务器充当调度者的角色，所有请求由它接收，根据每台服务器的负载情况将请求进行分发
    - 最小连接数
    - Ribbon算法
    - 一致性hash算法
        - 通过构建环状的Hash空间代替线性的Hash空间
        - 进行两次映射：第一次计算每个节点的Hash值，作为他们在环上的位置；第二次给每个Key计算Hash值，沿着顺时针方向找到环上第一个节点
            - 容易造成数据倾斜问题（顺时针下导致每个节点承载的请求量不相同）
        - 引入虚拟节点：解决服务崩溃造成的雪崩问题（一台机器崩溃，它原本负责的请求瞬间倾压在它的邻近机器上）、一个节点将会映射多个虚拟节点
        - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/6887AC6B-3D0F-5345-9461-0D60AF4A4495.png)
        - 简单代码实现
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/78448189-5F85-0541-9120-F3C1A2B0A3FE.png)

8. 分布式会话
    - Session Stick（nginx实现请求路由）：只需要保证每次请求都到同一台服务，就和单机一样了，但是存在问题，一旦服务器宕机或者重启，session数据丢失
    - Session复制：每台应用服务器，都对session会话数据进行保存，但是容易带来网络开销，某个session数据发生变化，就需将session同步至整个集群，并且session数据一多，则每台机器保存的session数据占用内存情况严重
    - Session集中管理：使用专门的服务管理session数据，每台服务从专门的session管理服务中获取会话数据，但是随之而来的是读写session带来的网络延迟问题，如果集中管理session的服务宕了，则会影响整个应用集群
    - nginx的ip绑定策略：同一ip只能在指定的同一个机器访问（无法支持负载均衡）
    - token代替session机制

9. 消息队列
    - 思考点：什么是消息队列？为什么要用？优劣点是什么？每种消息队列框架各自使用的场景是什么？

10. 分布式常见问题
    - 基于数据库实现分布式锁
        - 基于数据库的增删
            - 创建一张锁的表主要包含下列字段：方法名，时间戳等字段
            - 方法名具有唯一性约束
        - 基于数据库的排他锁
            - 查询语句添加“for update”，加上排他锁
    - 基于缓存，实现分布式锁，如redis
        - SETNX
            - 多个进程执行以下Redis命令：SETNX lock.id &lt;current Unix time + lock timeout + 1&gt;
            - 返回1，说明该进程获得锁，SETNX将键 lock.id 的值设置为锁的超时时间，当前时间 +加上锁的有效时间。
            - 返回0，说明其他进程已经获得了锁，进程不能进入临界区。进程可以在一个循环中不断地尝试 SETNX 操作，以获得锁
            - 如果某个进程突然与Redis断开了链接，锁没有及时释放的话，会导致死锁的发生
                - 在使用 SETNX 获得锁时，我们将键 lock.id 的值设置为锁的有效时间，线程获得锁后，其他线程还会不断的检测锁是否已超时，如果超时，等待的线程也将有机会获得锁
                - 在重新获得锁时，每个进程执行GETSET lock.id &lt;current Unix timestamp + lock timeout + 1&gt;，通过返回的oldValue进行判断锁是否超时，如果小于当前时间则进程获得锁，否则继续等待
                - 如果redis发生故障时，会导致整个分布式锁失效
            - Redis2.8之前，setnx指令与expire指令是分开执行的，不是组合的原子指令
            - 存在超时问题（事务执行的时间超过了加锁与释放锁之间的时间跨度，导致第一个事务还在执行时，第二个事务就获取到了锁，当第二个事务还在运行时，第一个事务由于执行完毕，将锁释放了，这个时候第三个事务获取到了锁）
                - 解决方法：设置随机参数，在释放锁时先对随机参数进行匹配，匹配一致删除key，用lua脚本将多个指令保证原子性运行
            - 可重入性（不推荐）
        - Redlock
            - 互斥、不能死锁
            - 尝试在大多数节点（n/2 + 1）进行加锁操作（存在创建锁的超时时间），创建失败的话依次删除锁
            - 在集群环境下，如果主节点挂掉时，从节点会取而代之，导致锁还没来得及同步到从节点，主节点突然挂掉了，导致系统中同一把锁被两个客户端同时持有
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/F4192F96-0670-6D49-8F00-4229C352DE9B.png)
            - 还是采用了大多数机制，加锁时向过半节点发送set(key, value, nx=True, ex=xxx)指令，只要过半节点set成功，那就认为加锁成功；释放锁时需要向所有节点发送del指令（同时还要考虑出错重试、时钟飘逸等问题）
    - 基于Zookeeper实现分布式锁
        - 为什么要采用临时节点
            - 避免创建锁的节点宕机导致锁无法释放而导致死锁现象的发生（一旦客户端与zookeeper端开链接，锁随机释放）
        - 基于Zookeeper临时有序节点实现分布式锁，每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个（最先创建锁）。
        - 如果没有获取到锁，则对这个锁注册一个监听器去监听这个锁的状态，锁不存在的话竞争创建锁

11. Nacos中间件
    - 微服务
        - 注册中心的核心数据是服务的名字和它对应的网络地址，当服务注册了多个实例时，我们需要对不健康的实例进行过滤或者针对实例的一些特征进行流量的分配，那么就需要在实例上存储一些例如健康状态、权重等属性。随着服务规模的扩大，渐渐的又需要在整个服务级别设定一些权限规则、以及对所有实例都生效的一些开关，于是在服务级别又会设立一些属性。再往后，我们又发现单个服务的实例又会有划分为多个子集的需求，例如一个服务是多机房部署的，那么可能需要对每个机房的实例做不同的配置，这样又需要在服务和实例之间再设定一个数据级别。
        - Nacos提供了四层的数据逻辑隔离模型，用户账号对应的可能是一个企业或者独立的个体，这个数据一般情况下不会透传到服务注册中心。一个用户账号可以新建多个命名空间，每个命名空间对应一个客户端实例，这个命名空间对应的注册中心物理集群是可以根据规则进行路由的，这样可以让注册中心内部的升级和迁移对用户是无感知的，同时可以根据用户的级别，为用户提供不同服务级别的物理集群。再往下是服务分组和服务名组成的二维服务标识，可以满足接口级别的服务隔离。
        - model：service->cluster->instance；service中存储着cluster的信息数据
    - 系统架构
        - 整体架构信息
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/5EF7C9B4-9036-824E-94E3-FBC703B79073.png)
        - 逻辑架构以及组件
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/762213F6-44AA-1941-BEAE-60E0B5A9A592.png)
        - 服务发现中心数据结构模型
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/358BE10C-5AC4-F54E-B833-3457BC16934D.png)
    - 概念
        - 配置快照（Configuration Snapshot）：客户端SDK会在本地生成一个配置的快照（为了缓解数据库查询压力问题）
        - 服务元数据：服务端点、服务标签、服务版本号、服务实例权重、路由规则、安全策略等描述服务等数据
        - 配置：在系统开发过程中通常会将一些需要变更的参数、变量等从代码中分离出来独立管理，以独立的配置文件的形式存在
        - 命名空间：通过设置不同的命名空间达到多环境隔离
        - 实例：在nacos中，实例为定义为提供一个或多个服务的具有可访问网络地址（IP:Port）的进程
        - 环境隔离的实现
            - Endpoint
                - 网络中 32 位的 IPV4 可以划分为很多网段，如192.168.1.0/24，并且一般中大型的企业都会有网段规划，按照一定的用途划分网段。我们可以利用这个原理做环境隔离，即不同网段的 IP 属于不同的环境，如192.168.1.0/24属于环境A， 192.168.2.0/24属于环境B等
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/5303FAC4-3FE9-1B4C-82D3-68B0F7EB50AD.png)
        - DNS协议为基础的服务发现
            - 具备天然的跨语言特性
            - Nacos的DNS-F服务发现设计
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/CC9197C3-40A4-E645-8A64-45315D92F216.png)
    - AP模式与CP模式
        - AP模式
            - AP模式下只支持注册临时实例；无法编辑服务的元数据等非实例级别的数据，允许创建一个默认的配置服务；同时实例注册前不再需要注册服务了，因此此时服务会被降级为一个简单的字符串标识，不存储任何属性
            - 为什么AP模式下只支持注册临时实例？
                - 由于AP模式，追求的是最终一致性，因此临时实例采用客户端心跳上报自身的健康状态信息，因此当客户端重新上线后，相当于数据补偿
        - CP模式
            - CP模式下只支持注册持久化实例；此时以Raft协议为集群运行模式；可以编辑服务的元数据，在实例注册之前必须先注册服务，否则返回错误
            - 为什么CP模式下只支持注册持久化实例？
                - 由于CP模式，追求的是强一致性，因此实例采用的是服务端向客户端发送心跳数据包，判断实例是否存活
                - 对于数据存储等服务实例需要采用持久化的实例进行注册（因为这些基础服务无法说依赖于nacos-sdk进行启动，也就无法采用客户端上报心跳的形式）
    - 一致性策略
        - 清除历史配置的（DumpService）
            - 历史表清理任务是地址列表中第一个ip去做，保证执行的唯一性；cluster.conf 中如果我们ip是第一个，那就去执行清理超过30天历史记录的任务，挂机器一定要尽快摘除的，摘除之后第一个ip就变了。 新的第一ip继续执行删除任务；挂机器一般不能超过12小时处理的，不然水平消息堆积太多，会导致其他节点fullgc。
        - DistroConsistencyServiceImpl（类似于Eureka的一致性策略、gossip）
            - 主要成员DataStore、TaskDispatcher、DataSyncer
        - RaftConsistencyServiceImpl（Raft协议）
    - 服务发现
    - 配置管理
        - Nacos 的客户端 SDK 会在本地生成配置的快照。当客户端无法连接到 Nacos Server 时，可以使用配置快照显示系统的整体容灾能力。配置快照类似于 Git 中的本地 commit，也类似于缓存，会在适当的时机更新，但是并没有缓存过期（expiration）的概念。
    - 多租户的架构设计（nacos是采用namespace对标租户概念）
        - 其设计的重点为如何在一个系统中确保各用户间的数据的隔离性
        - 三种数据隔离方案
            - 独立数据库：一个租户一个数据库；虽然说数据隔离级别最高，安全性最好，可满足不同租户的特殊需求，出现故障其数据恢复也比较简单；但是相应的成本也上去了
            - 共享数据库，独立Schema：多个租户共享一个DataBase，但是每个租户一个schema，一定程度的逻辑数据隔离，但并不是完全隔离；每个数据库支持更多的租户数量；但是出现故障的话数据恢复比较困难，因为涉及其他租户的数据信息
            - 共享数据库，共享schema，共享数据表：在表中添加tenantID的多租户数据字段，共享程度最高，但是数据隔离级别最低，每次插入一条数据都需要带上租户的唯一标识信息；但是造成需要大量的安全代码开发，数据备份、恢复困难
    - Nacos的环境隔离机制
    - DNS发现的实现原理

12. Eureka
    - Eureka总体架构（https://www.infoq.cn/article/jlDJQ*3wtN2PcqTDyokh）
        - Eureka 注册中心部署在多个机房的架构图，这正是他高可用性的优势
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/7B23E1C2-C7CF-DE40-9543-E564EE8F9A68.png)
    - eureka提供了region和zone两个概念来进行分区
        - region：可以简单理解为地理上的分区，比如亚洲地区，或者华北地区，再或者北京等等，没有具体大小的限制。根据项目具体的情况，可以自行合理划分region。
        - zone：可以简单理解为region内的具体机房，比如说region划分为北京，然后北京有两个机房，就可以在此region之下划分出zone1,zone2两个zone。
        - 架构图
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/6FA37D35-9F3E-9B42-BB13-4A3F818432B9.png)

13. 分布式服务——链路追踪（zipkin）
    - APM的原理实现
    - 流程图
        - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/9FFAFC8C-2374-F549-BDAA-B4745B32BD25.png)
    - 各业务系统在彼此调用时，将特定的跟踪消息传递至zipkin，zipkin在收集到跟踪信息后将其聚合处理、存储、展示等，用户可通过web UI方便获得网络延迟、调用链路、系统依赖
    - 四个模块
        - Collector模块
        - Storage模块
        - API(Query)模块
        - Web提供简单web界面
    - 采样器
        - 可以根据trace-id进行判断当前tace是否需要被上报至zipkin中，一个tracing记录这当前整个调用链路的信息数据
        - TraceContext中定义了两个接口：Injector以及Extractor
            - Injector用于将TraceContext中的各种数据注入到Carrier中，
            - Extractor用于在Carrier中提取TraceContext相关信息或者采样标记信息
        - 如何在Web项目中实现链路追踪的

14. Sentinel
    - 工作原理：在 Sentinel 里面，所有的资源都对应一个资源名称（resourceName），每次资源调用都会创建一个 Entry 对象。Entry 可以通过对主流框架的适配自动创建，也可以通过注解的方式或调用 SphU API 显式创建。Entry 创建的时候，同时也会创建一系列功能插槽（slot chain），这些插槽有不同的职责
        - NodeSelectorSlot 负责收集资源的路径，并将这些资源的调用路径，以树状结构存储起来，用于根据调用路径来限流降级；
        - ClusterBuilderSlot 则用于存储资源的统计信息以及调用者信息，例如该资源的 RT, QPS, thread count 等等，这些信息将用作为多维度限流，降级的依据；
        - StatisticSlot 则用于记录、统计不同纬度的 runtime 指标监控信息；
        - FlowSlot 则用于根据预设的限流规则以及前面 slot 统计的状态，来进行流量控制；
        - AuthoritySlot 则根据配置的黑白名单和调用来源信息，来做黑白名单控制；
        - DegradeSlot 则通过统计信息以及预设的规则，来做熔断降级；
        - SystemSlot 则通过系统的状态，例如 load1 等，来控制总的入口流量
            - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/D7DD8088-FCAC-FD40-B139-CA28C474E74C.png)
    - 流量控制
        - 限流的直接表现是在执行 Entry nodeA = SphU.entry(resourceName) 的时候抛出 FlowException异常
        - 一条限流规则主要由下面几个因素组成，我们可以组合这些元素来实现不同的限流效果：
            - resource：资源名，即限流规则的作用对象
            - count: 限流阈值
            - grade: 限流阈值类型（QPS 或并发线程数）
            - limitApp: 流控针对的调用来源，若为 default 则不区分调用来源
            - strategy: 调用关系限流策略
            - controlBehavior: 流量控制效果（直接拒绝、Warm Up、匀速排队）
        - QPS流量控制（直接拒绝、Warm Up、匀速排队）
            - 直接拒绝：默认的使用方式，当QPS超过任意规则的阈值后，新的请求就会被立即拒绝，拒绝方式为抛出FlowException。这种方式适用于对系统处理能力确切已知的情况下，比如通过压测确定了系统的准确水位时
            - Warm Up：即预热/冷启动方式。当系统长期处于低水位的情况下，当流量突然增加时，直接把系统拉升到高水位可能瞬间把系统压垮。通过"冷启动"，让通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，给冷系统一个预热的时间，避免冷系统被压垮
            - 匀速排队：严格控制请求通过的间隔时间，也即是让请求以均匀的速度通过，对应的是漏桶算法；要用于处理间隔性突发的流量，例如消息队列。想象一下这样的场景，在某一秒有大量的请求到来，而接下来的几秒则处于空闲状态，我们希望系统能够在接下来的空闲期间逐渐处理这些请求，而不是在第一秒直接拒绝多余的请求
        - 基于调用关系的流量控制
            - 调用关系包括调用方、被调用方；一个方法又可能会调用其它方法，形成一个调用链路的层次关系。Sentinel 通过 NodeSelectorSlot 建立不同资源间的调用的关系，并且通过 ClusterNodeBuilderSlot 记录每个资源的实时统计信息

15. 云原生相关
    - Etcd
        - 数据的传输通道类型：stream以及pipeline
        - 数据存储
            - 集群节点之间的网络拓扑图是一个任意两个节点之间均有长连接互相连接的网状结构
        - WAL
            - 什么是WAL
                - 预写式日志(WAL)是一种确保数据完整性的标准方法。 有关它的详细描述可以在大多数(如果不是全部的话)有关事务处理的书中找到。简而言之， WAL的中心思想是对数据文件的修改(它们是表和索引的载体) 必须只能发生在这些修改已经记录到日志之后，也就是说， 在描述这些变化的日志记录刷写到永久存储器之后。如果我们遵循这个过程， 那么就不需要在每次事务提交的时候都把数据页刷写到磁盘， 因为在出现崩溃的情况下可以用日志来恢复数据库： 任何尚未应用于数据页的修改都可以先从日志记录中重做(这叫向前滚动恢复，也叫 REDO)。
                - 使用WAL显著地减少了磁盘写的次数，因为只有日志文件需要刷写到磁盘以保证事务被提交， 而不是事务修改的所有数据文件都需要刷写到磁盘。日志文件是顺序写的，所以同步日志的开销要远比刷写数据页的开销小。 尤其对于有很多修改不同数据存储位置的小事务的服务而言更是如此。另外，当服务器正在处理许多小的并发事务时， 日志文件的一个fsync足以提交许多事务
            - 处理一条Entry的大致流程
                - ![shilitu](https://github.com/chuntaojun/Java_Note/blob/master/distribution/image/AD05724B-8553-0242-874A-1B6C340C31BD.png)